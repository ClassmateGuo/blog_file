
## 前提

> 在切片集群中，数据会按照一定的分布规则分散到不同的实例上保存。比如，在使用 `Redis Cluster` 或 `Codis` 时，数据都会先按照 `CRC` 算法的计算值对 `Slot`（逻辑槽）取模，同时，所有的 `Slot` 又会由运维管理员分配到不同的实例上。这样，数据就被保存到相应的实例上了。

虽然这种方法实现起来比较简单，但是很容易导致一个问题：数据倾斜。

数据倾斜有两类：
+ 数据量倾斜：在某些情况下，实例上的数据分布不均衡，某个实例上的数据特别多。


+ 数据访问倾斜：虽然每个集群实例上的数据量相差不大，但是某个实例上的数据是热点数据，被访问得非常频繁。
如果发生了数据倾斜，那么保存了大量数据，或者是保存了热点数据的实例的处理压力就会增大，速度变慢，甚至还可能会引起这个实例的内存资源耗尽，从而崩溃。这是我们在应用切片集群时要避免的。

## 数据量倾斜的成因和应对方法

### bigkey 导致倾斜

第一个原因是，某个实例上正好保存了 `bigkey`。`bigkey` 的 `value` 值很大（`String` 类型），或者是 `bigkey` 保存了大量集合元素（集合类型），会导致这个实例的数据量增加，内存资源消耗也相应增加。

而且，`bigkey` 的操作一般都会造成实例 `IO` 线程阻塞，如果 `bigkey` 的访问量比较大，就会影响到这个实例上的其它请求被处理的速度。

> 为了避免 `bigkey` 造成的数据倾斜，一个根本的应对方法是，我们在业务层生成数据时，要尽量避免把过多的数据保存在同一个键值对中。如果 `bigkey` 正好是集合类型，还有一个方法，就是把 `bigkey` 拆分成很多个小的集合类型数据，分散保存在不同的实例上。



### Slot 分配不均衡导致倾斜

如果集群运维人员没有均衡地分配 `Slot`，就会有大量的数据被分配到同一个 `Slot` 中，而同一个 `Slot` 只会在一个实例上分布，这就会导致，大量数据被集中到一个实例上，造成数据倾斜。

以 `Redis Cluster` 为例，来介绍下 `Slot` 分配不均衡的情况。

> `Redis Cluster` 一共有 `16384` 个 `Slot`，假设集群一共有 `5` 个实例，其中，实例 1 的硬件配置较高，运维人员在给实例分配 `Slot` 时，就可能会给实例 `1` 多分配些 `Slot`，把实例 `1` 的资源充分利用起来。
>
> 但是，我们其实并不知道数据和 `Slot` 的对应关系，这种做法就可能会导致大量数据正好被映射到实例 `1` 上的 `Slot`，造成数据倾斜，给实例 `1` 带来访问压力。

为了应对这个问题，可以通过运维规范，在分配之前，就要避免把过多的 `Slot` 分配到同一个实例
查看 `Slot` 分配，如果是 `Redis Cluster`，就用 `CLUSTER SLOTS` 命令；如果是 `Codis`，就可以在 `codis dashboard` 上查看。
```
127.0.0.1:6379> cluster slots
1) 1) (integer) 0
   2) (integer) 4095
   3) 1) "192.168.10.3"
      2) (integer) 6379
2) 1) (integer) 12288
   2) (integer) 16383
   3) 1) "192.168.10.5"
      2) (integer) 6379
```
如果某一个实例上有太多的 `Slot`，可以使用迁移命令把这些 `Slot` 迁移到其它实例上。在 `Redis Cluster` 中，我们可以使用 `3` 个命令完成 `Slot` 迁移。

1. `CLUSTER SETSLOT`：使用不同的选项进行三种设置，分别是设置 `Slot` 要迁入的目标实例，`Slot` 要迁出的源实例，以及 `Slot` 所属的实例。
2. `CLUSTER GETKEYSINSLOT`：获取某个 `Slot` 中一定数量的` key`。
3. `MIGRATE`：把一个 `key` 从源实例实际迁移到目标实例。

### Hash Tag 导致倾斜

`Hash Tag `是指加在键值对 key 中的一对花括号{}。这对括号会把 key 的一部分括起来，客户端在计算 `key` 的 `CRC16` 值时，只对 `Hash Tag` 花括号中的 `key` 内容进行计算。如果没用 `Hash Tag` 的话，客户端计算整个 `key` 的 `CRC16` 的值。

**举个例子：**

> 假设 `key` 是 `user:profile:3231`，把其中的 `3231` 作为 `Hash Tag`，此时，`key` 就变成了 `user:profile:{3231}`。当客户端计算这个 `key `的 `CRC16` 值时，就只会计算 `3231` 的 `CRC16` 值。否则，客户端会计算整个“`user:profile:3231`”的 `CRC16` 值。
>
> 使用 `Hash Tag` 的好处是，如果不同 `key` 的 `Hash Tag` 内容都是一样的，那么，这些 key 对应的数据会被映射到同一个 `Slot` 中，同时会被分配到同一个实例上。

![相同Slot数据映射](.pic/2023-03-29-%E7%9B%B8%E5%90%8CSlot%E6%95%B0%E6%8D%AE%E6%98%A0%E5%B0%84.png)

**使用 Hash Tag 的潜在问题**：

就是大量的数据可能被集中到一个实例上，导致数据倾斜，集群中的负载不均衡。那么，该怎么应对这种问题呢？就需要在范围查询、事务执行的需求和数据倾斜带来的访问压力之间，进行取舍了。

> 建议是，如果使用 `Hash Tag` 进行切片的数据会带来较大的访问压力，就优先考虑避免数据倾斜，最好不要使用 `Hash Tag` 进行数据切片。因为事务和范围查询都还可以放在客户端来执行，而数据倾斜会导致实例不稳定，造成服务不可用。

## 数据访问倾斜的成因和应对方法

热点数据通常是一个或几个数据，所以，直接重新分配 `Slot` 并不能解决热点数据的问题。

通常来说，热点数据以服务读操作为主，在这种情况下，可以采用热点数据多副本的方法来应对。

> 这个方法的具体做法是，把热点数据复制多份，在每一个数据副本的 `key` 中增加一个随机前缀，让它和其它副本数据不会被映射到同一个 `Slot` 中。这样一来，热点数据既有多个副本可以同时服务请求，同时，这些副本数据的 key 又不一样，会被映射到不同的 `Slot` 中。在给这些 `Slot` 分配实例时，我们也要注意把它们分配到不同的实例上，那么，热点数据的访问压力就被分散到不同的实例上了。
>
> 需要注意一个地方，热点数据多副本方法只能针对只读的热点数据。如果热点数据是有读有写的话，就不适合采用多副本方法了，因为要保证多副本间的数据一致性，会带来额外的开销。

对于有读有写的热点数据，就要给实例本身增加资源了，例如使用配置更高的机器，来应对大量的访问压力。